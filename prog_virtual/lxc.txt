# #!/bin/bash
# vim:ts=2
# program: Using to note the knowledge about lxc
# made by: Engells
# date: Oct 8, 2025
# content: mount directory in tmpfs mode



Working environment
====================
user: admit
passed: name_at_lxc_~year

Build base environment
  set ssh
    lst-start -n xxxx && lxc-attach -n xxxx (host)
    apt update && apt install openssh-server && apt autoremove && apt remove && apt autoclean && apt clean
    set passwd for root and ubuntu
    log-out and restart cntr
  Change default user form ubuntu to admit
    Add temp super user
      send ssh-key to cntr for ubuntu account (host)
      ssh ubuntu@cntr_ip
      sudo groupadd -g 1001 admin && sudo useradd -u 1001 -g 1001 -m -G users,audio,video,render -s /bin/bash -d /home/admin admin
      sudo passwd admin
      edit /etc/sudoers.d/90-incus => add admit
      log-out
    Add admit account
      send ssh-key to cntr for admin account (host)
      ssh admin@cntr_ip
      sudo userdel -r -f ubuntu && sudo groupdel -f ubuntu
      sudo groupadd -g 1000 admit && sudo useradd -u 1000 -g 1000 -m -G users,audio,video,render -s /bin/bash -d /home/admit admit
      sudo passwd admit
      edit /etc/sudoers.d/90-incus => add admit
      edit /etc/ssh/sshd_conf.d/xxxx.conf => set sshd configure
      log-out
    Delete temp super user
      send ssh-key to cntr for admit account (host)
      ssh admit@cntr_ip
      sudo userdel -r -f admin && sudo groupdel -f admin
      edit /etc/sudoers.d/90-incus => keep only admit

Build chinese environment
  edit /var/lib/locales/supported.d/[en,zh-hant] => enable en_US.UTF-8 UTF-8 and zh_TW.UTF-8 UTF-8
  edit /etc/locale.gen => enable en_US.UTF-8 UTF-8 and zh_TW.UTF-8 UTF-8
  sudo apt install fontconfig localepurge language-pack-zh-hans
  sudo locale-gen --purge		# sudo locale-gen zh_TW.UTF-8
  sudo dpkg-reconfigure localepurge # optional
  sudo apt install fonts-noto-cjk fonts-noto-cjk-extra fonts-noto-mono fonts-noto-color-emoji ;; fc-cache -f -v

Package install
  sudo apt install fontconfig xauth dbus dbus-x11  # libcanberra-gtk-module libcanberra-gtk3-module
  sudo apt install gnupg
  wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmour -o /etc/apt/keyrings/packages.chrome.gpg
  sudo sh -c 'echo "deb [arch=amd64 signed-by=/etc/apt/keyrings/packages.chrome.gpg] http://dl.google.com/linux/chrome/deb/ stable main" > /etc/apt/sources.list.d/google.list'
  sudo apt update && sudo apt install google-chrome-stable
  sudo apt --fix-broken install libnotify4 libxss1 libsecret-1-0 libsecret-common
  # install Firefox via Mozilla Repo
  # wget -q https://packages.mozilla.org/apt/repo-signing-key.gpg -O - | sudo tee /etc/apt/keyrings/packages.mozilla.org.asc > /dev/null
  # echo "deb [signed-by=/etc/apt/keyrings/packages.mozilla.org.asc] https://packages.mozilla.org/apt mozilla main" | sudo tee /etc/apt/sources.list.d/mozilla.list
  # nano /etc/apt/preferences.d/mozilla
  # Package: firefox*
  # Pin: origin packages.mozilla.org
  # Pin-Priority: 1001
  #
  # Package: firefox*
  # Pin: release o=Ubuntu
  # Pin-Priority: -1
  #
  # install download manager
  # wget https://github.com/agalwood/Motrix/releases/download/v1.8.19/Motrix_1.8.19_amd64.deb && sudo dpkg -i Motrix_1.8.19_amd64.deb
  # sudo apt update && sudo apt install openjdk-18-jre --no-install-suggests --no-install-recommends
  # chmod +x JDownloader2Setup_unix_nojre.sh ; ./JDownloader2Setup_unix_nojre.sh
  # https://gist.github.com/leosuncin/50367b3c905fe8699b13bd85ad26071b
  # https://linuxconfig.org/ubuntu-debian-jdownloader-installation-howto

Network
  build thaubr0 in host, and link veth to thaubr0
  edit /etc/netplan/10-lxc.yaml

Physiacal hard disk partition
  work in privileged mode

Public directory and tmpfs
  lxc.mount.entry = tmpfs var/log none bind 0 0
  lxc.mount.entry = tmpfs var/tmp none bind 0 0
  lxc.mount.entry = tmpfs mnt/tmpfs none bind 0 0
  lxc.mount.entry = /tmp/z_cache_[lxc_container_name] home/admit/.cache none bind 0 0
  lxc.mount.entry = /tmp/z_downloads home/admit/downloads none bind 0 0

Set log file size limit and set shell command alias
  edit /etc/logrotate.conf or /etc/logrotate.d/xxxx.conf
    size 10M
  edit %HOME/.bashrc or $HOME/.zshrc
    alias la='ls -al --color'
    alias clnapt='sudo apt autoremove && sudo apt remove && sudo apt autoclean && sudo apt clean'

Backup container, must run tar as root
  sudo tar -pcvf xxxx.tar xxxx/ && 7z a -mx=9 xxxx.tar.7z xxxx.tar # -p to keep same permission in container rootfs
  # sudo tar -cvpf - xxxx/ | 7z a -mx=9 -si xxxx.tar.7z ;; 7z x -so xxxx.tar.7z | sudo tar xvpf -




常用指令
====================
啟動 Container
  lxc-start -n name_of_machine -d
  lxc-restart -n name_of_machine       # 重啟  Container
  lxc-execute -n name_of_machine cmd   # 啟動 Container 並執行指令

查詢 Container
  lxc-ls -f                            # 查詢 Container 名單
  lxc-info -n name_of_machine          # 查詢 Container 詳細資料

登入 Conatiner
  lxc-attach -n name_of_machine
  lxc-console -n name_of_machine       # 終端機模式，Ctl+a q 退出 console
  ssh -X user@ip_of_container          # SSH 模式登入可執行視窗時程

停止 Container
  lxc-stop -n name_of_machine

複製 Container
  lxc-copy -n old_machine_name -N new_machine_name

暫停及恢復 Container
  lxc-freeze -n name_of_machine
  lxc-unfreeze -n name_of_machine

刪除 Container
  lxc-destroy -n name_of_machine

查詢 lxc 版本
  lxc-start --version

下載範本建立 Container
  lxc-create -t download -n name_of_machine -- -d ubuntu -r jammy -a amd64
  # lxc-create -t download -n xxxx -- --dist archlinux --release current --arch amd64

管理 lxc servoce
  systemctl start lxc ;; systemctl enable lxc ;; systemctl restart lxc




Containers share folders
====================
create folder /path/to/share in host
chmod 777 /path/to/share in host
create folder /path/to/mount in container
edit /path/to/container/config
  lxc.mount.entry = /home/virt/share mnt/share none bind 0 0
    # /home/virt/share <= /path/to/share in host
    # mnt/share <= /path/to/mount in container
*****
lxc.mount.entry = /dev/sde dev/sde none bind,optional,create=file           # mount a device in container
lxc.mount.entry = /dev/mapper/lvmfs-home-partition home ext4 defaults 0 2   # mount a filesystem in container




X in container
====================
Method 1: ssh X forwarding
----------
安裝 openssh
  apt install openssh-server
  touch /path/to/home/.Xauthority
  add "X11UseLocalhost no" to /etc/ssh/sshd_config

ssh 連線
  ssh -YC4 ubuntu@10.0.3.101
  # run this command in host
  # or ssh -v -X -C -o CompressionLevel=9 user_name@remote_computer_ip ;; ssh -v -X ubuntu@10.0.3.101

安裝瀏覽器
  /usr/bin/firefox
  # need packages: xauth dbus dbus-x11 libcanberra-gtk-module libcanberra-gtk3-module
  # chromium: ref to section of Working environment, mesa-utils? alas-utils?

Refs:
  lxc 容器內如何運行 Firefox :: http://www.rendoumi.com/lxcrong-qi-nei-ru-he-yun-xing-firefox/
  github Roadmaster/google-chrome-lxc.sh :: https://gist.github.com/Roadmaster/0de007826485d0e5a9c856171a9a1e9c


Method 2: Xpra(persistent remote display server and client for forwarding application，screen for X11，tset sucessful)
----------
在 host 及 cntr(container) 各自安裝 xpra
  pacman install -S xpra or apt -y install xpra

應用 1: cntr 啟用 xpra server，host 以 xpra client 連線
  xpra start :101 (cntr)
  xpra attach ssh://user_name@xxx.xxx.xxx.xxx:/port/101 1>/dev/null 2>/dev/null & (local)
  xpra detach ssh://user_name@xxx.xxx.xxx.xxx:/port/101 (local)
  xpra stop :101 (cntr)

應用 2: 在本機啟用 xpra，可直接連線 remote
  xpra start ssh://user:passwd@host_ip:port/display --start=xterm                     # 在本機啟用 xpra 並執行 remote xterm 終端機
  xpra start ssh://... --start-child="xterm" --exit-with-children=yes                 # 在本機啟用 xpra 並執行 remote xterm 終端機，程式完成時，xpra 隨之關閉
  xpra start-desktop ssh://... --start-child="xterm" --exit-with-children=yes         # 在本機啟用 xpra 並以桌面方式執行 remote xterm 終端機
  xpra start-desktop ssh://... --start-child=xfce4-session --exit-with-children=yes   # 在本機啟用 xpra 並執行 remote xface 桌面環境
  xpra start --bind-tcp=127.0.0.1:5900 --html=on --start=xterm                        # 在本機啟用 xpra 並執行 xterm 終端機，可用瀏覽器按 ip 連線
  xpra start --bind-tcp=127.0.0.1:5900 --html=on --start=xfce4-session                # 在本機啟用 xpra 並執行 xface 桌面環境，可用瀏覽器按 ip 連線

應用 3: cntr 使用 ibus 輸入法
  apt install ibus-pinyin
  xpra start ssh:user_name@remote_ip:101 --exit-with-children=yes --speaker=off --webcam=no \
    --input-method=IBus --start-child="ibus-setup"            # 設置 ibus
  xpra start ssh:user_name@remote_ip:101 --exit-with-children=yes --start-child=firefox --speaker=off --webcam=no \
    --input-method=IBus --start-child="ibus-daemon -x -d -r"  # 執行 firefox 及 ibus

其他指令或參數
  xpra list

Refs:
  Xpra: GUI 界的 screen/tmux :: https://nyllep.wordpress.com/2020/09/02/xpra-gui-%E7%95%8C%E7%9A%84-screen-tmux/
  Arch Wiki :: https://wiki.archlinux.org/title/Xpra
  X 持久化远程应用 Xpra 快速起步 :: https://cloud-atlas.readthedocs.io/zh-cn/latest/linux/desktop/xpra/xpra_startup.html
  xpra 安装和使用 -  :: https://blog.csdn.net/qq_38781075/article/details/108240132
  Xpra 使用入門 - 台部落 :: https://www.twblogs.net/a/5d01c03ebd9eee14644f9d33
  Xpra 入门 :: https://openwares.net/2020/02/04/xpra-intro/


Method 3: xvfb + vnc server(tset sucessful)
----------
在容器執行 xvfb 及 vnc server
  apt install xvfb or pacman -S xorg-server-xvfb
    #安裝 xvfb(virtual framebuffer X server for X Version 11)
  Xvfb :9 -screen 0 1280x960x24 -listen tcp -ac +extension GLX +extension RENDER &
    # :9 為輸出螢幕編號；-screen 1280x960x24 為螢幕大小及色彩；-listen tcp 監聽 tcp 連接；-ac 允許任何設備連接至 Xvfb 伺服器；
    # +extension 參數加載 GLX 和 RENDER 模組，支持 OpenGL 和圖形渲染； & 表示背景執行
  export DISPLAY=:9
  apt install x11vnc fontconfig fonts-noto-cjk fonts-noto-mono fonts-noto-color-emoji
  x11vnc -display :9 -forever -shared -rfbport 5900 -passwd 123456 &
    # -display 參數指定為輸出螢幕編號，預設為 :0； -forever 參數表示連接中斷後 x11vnc 伺服器維持運作； -shared 參數允許多客戶端同時連接；
    # -rfbport 參數指定 VNC 服務端口，預設為 5900； -passwd 參數設置密碼

在主機以 vnc clent 連接
  xxx.xxx.xxx.xxx:5900

補充
  可以把 Xvfb 運行在宿主機上，搭配容器內設定 export DISPLAY=host_ip:9
  若 X server 與 client（GUI程式）不是運行在一個系統上，虛擬 X server 那邊需執行：xhost +<另一系統的IP>
  新開顯示器並執行圖形應用程式，程式完成時，xvfb 隨之關閉，xvfb-run --auto-servernum --server-args="-screen 0, 1920x1080x24" nw script.js
    ps -ef | grep auth ; x11vnc -display :99 -auth /tmp/xvfb-run.RCwemo/Xauthority  # auth 位置為前述 ps 指令所查得

Refs:
  Docker 容器应用可视化 - VNC 方式 :: https://blog.csdn.net/weixin_56291477/article/details/131856950
  docker + xrfb-run + vnc：Docker 里开虚拟屏幕运行带 GUI 程序 :: https://www.cnblogs.com/tugeler/p/16791259.html
  Arch Wiki :: https://wiki.archlinux.org/title/X11vnc


Method 4: Others
----------
Xephyr(嵌套的 X 視窗，可替代 Xnest，tset sucessful, not preferred)
  install: pacman -S xorg-server-xephyr or apt install xserver-xephyr nxagent xpra
  run: Xephyr :1 -br -ac -noreset -host-cursor -screen 1280x720

Xdummy
  install: pacman install -S xf86-video-dummy or apt install xserver-xorg-video-dummy
  config:  /usr/share/X11/xorg.conf.d/xorg.conf




Device passthrough
====================
GPU passthrough
----------
1.在 host 安裝 GPU 驅動程式
2.將 host 使用者加入 video 及 render 群組
  sudo usermod -aG video,render name_of_user
3.在 host 設定 lxc container 運作環境
  編輯 /etc/subgid，加入
    name_of_user:video_group_id:1    # video_group_id = $(grep video /etc/group)
    name_of_user:render_group_id:1   # video_group_id = $(grep render /etc/group)
  編輯 /path/to/container/config，加入
    #lxc.cgroup.relative = 1
    lxc.cgroup2.devices.allow = c xxx:xxx rwm    # c xxx:xxx =$(ls -al /dev/dri | grep card)
    lxc.cgroup2.devices.allow = c xxx:xxx rwm    # c xxx:xxx =$(ls -al /dev/dri | grep render)
    lxc.mount.entry = /dev/dri/card0 dev/dri/card0 none bind,optional,create=file
    lxc.mount.entry = /dev/dri/renderD128 dev/dri/renderD128 none bind,optional,create=file
    lxc.idmap = g xxx xxx 1
4.在 container 安裝 GPU 驅動程式
  sudo pacman -S intel-media-driver linux-firmware-lts libva-utils or apt install i965-va-driver intel-media-va-driver mesa-va-drivers mesa-vulkan-drivers vainfo
  # ArchWiki Hardware video acceleration :: https://wiki.archlinux.org/title/Hardware_video_acceleration
5.備註
  在 /path/to/container/config 加入 hook
    編輯 /path/to/container/config，加入
      lxc.autodev = 1
      lxc.hook.autodev = /path/to/container/mount_hook.sh
    編輯 /path/to/container/mount_hook.sh
      mkdir -p ${LXC_ROOTFS_MOUNT}/dev/dri
      mknod -m 666 ${LXC_ROOTFS_MOUNT}/dev/dri/card0 c xxx xxx
      mknod -m 666 ${LXC_ROOTFS_MOUNT}/dev/dri/renderD128 c xxx xxx
      # mknod 指定在 container 無法執行


Sound device passthrough
----------
1.在 host 安裝 sound 驅動程式，測試音效裝置
  aplay -L && speaker-test -D <name> -c 2  # 實測裝置為 plughw:CARD=PCH,DEV=0
2.將 host 使用者加入 audio 群組
  sudo usermod -aG audio name_of_user
3.在 host 設定 lxc container 運作環境
  編輯 /etc/subgid，加入 audio 群組的 gid mapping
    name_of_user:audio_group_id:1
    # admit:996:1  # audio_group_id = $(grep audio /etc/group)
  編輯 /path/to/container/config，加入
    lxc.cgroup2.devices.allow = c 116:* rwm    # c xxx:xxx =$(ls -al /dev/snd)
    lxc.mount.entry = /dev/snd dev/snd none bind,optional,create=dir
    lxc.idmap = g cntr_audio_id host_audio_id 1
4.在 container 安裝 sound 驅動程式 pulseaudio
  install alsa-utils && alsa-tools
  aplay /usr/share/sounds/alsa/Front_Left.wav
  edit /etc/pulse/default.pa  # 設置 host pulse 組態
    load-module module-alsa-sink device=plughw:CARD=PCH,DEV=0


NIC Passtrough
----------
1.在 host 建立 VF 網卡
  sudo ip addr add 192.168.2.12/24 label wlp0s20f3:1 dev wlp0s20f3
2.在 host 設定 lxc container 運作環境
  編輯 /path/to/container/config，加入
    lxc.net.0.type = phys
    lxc.net.0.link = wlp0s20f3:1
    lxc.net.0.name = wlp0s20f4
    # lxc.net.0.ipv4.address = 192.168.1.12/24
    # lxc.net.0.ipv4.gateway = 192.168.1.1
    lxc.net.0.flags = up
3.實測在 Unprivileged Mode 不支持 lxc.net.0.type = phys


Ref:
----------
Proxmox 中 LXC 容器中 docker 调用宿主机的显卡加速 :: https://ferrets.space/2022/03/17/proxmox...
Setting up LXC with Intel GPU (Proxmox), keyboard, mouse and audio :: https://blog.konpat.me/dev/2019/03/11/setting-up-lxc-for-intel-gpu-proxmox.html
LXC 直通顯示卡 :: https://wiki.freedomstu.com/books/proxmox-ve-%E8%99%9B%E6%93%AC%E7%B3%BB%E7%B5%B1%E8%A8%98%E9%8C%84/page/lxc-wRC
超详细，多图，简单 Jellyfin on LXC， Docke， NAS挂载  (个人记录) :: https://post.smzdm.com/p/awzw32rm/
LXC 容器简明教程 :: https://blog.lt2n.com/2023/04/29/LXC%E5%AE%B9%E5%99%A8%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/
LXC "直通"网卡 :: https://pvecli.xuan2host.com/lxc-network-bypass/
Running (Almost) Anything in LXC: Sound :: https://xahteiwi.eu/resources/hints-and-kinks/lxc-sound/
pulseaudio 一對一聲音串流超簡單 :: https://newtoypia.blogspot.com/2022/05/pulseaudio.html
Pipewire audio in Fedora container :: https://stackoverflow.com/questions/68973199/pipewire-audio-in-fedora-container
LXC 容器如何以本地方式运行 X Server(Xephyr) ::  https://www.yisu.com/zixun/9726.html
台部落 LXC之.conf配置文件詳解 :: https://www.twblogs.net/a/5d0a68c3bd9eee1e5c8157be
(old) LXC 1.0: Blog post series :: https://stgraber.org/2013/12/20/lxc-1-0-blog-post-series/ :: https://stgraber.org/2014/02/09/lxc-1-0-gui-in-containers/
(old) LXC Audio / Sound :: https://discuss.linuxcontainers.org/t/lxc-audio-sound/8582




NetWork Config
====================
edit /etc/default/lxc-net
  LXC_BRIDGE="lxcbr0"
  LXC_ADDR="172.25.3.1"
  LXC_NETMASK="255.255.255.0"
  LXC_NETWORK="172.25.3.0/24"
  LXC_DHCP_RANGE="172.25.3.2,172.25.3.254"
  LXC_DHCP_MAX="253"
sudo systemctl restart lxc-net
sed -i "/USE_LXC_BRIDGE/s/true/false/g" /etc/default/lxc-net    # disable auto-start lxcbr0


edit /var/lib/lxc/name_of_machine/config or $HOME/.local/share/lxc/name_of_machine/config
  lxc.network.link = lxcbr0


Ubuntu container with static IP
----------
container file: /etc/netplan/10-lxc.yaml
network:
  version: 2
  ethernets:
    eth0:
      addresses: [172.25.3.11/24]
      gateway4: 172.25.3.1
      dhcp4: no
      dhcp6: no
      optional: true
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]


CentOS container with static IP
----------
container file: /etc/sysconfig/network-scripts/ifcfg-eth0
  BOOTPROTO=static
  IPADDR=172.25.3.101
  NETMASK=255.255.255.0
  GATEWAY=172.25.3.1
  DNS1=172.25.3.1




Unprivileged Mode
====================
config files matching
  /etc/default/lxc       => keep the same
  /etc/default/lxc-net   => keep the same
  /etc/lxc/lxc.conf      => $HOME/.config/lxc/lxc.conf
  /etc/lxc/default.conf  => $HOME/.config/lxc/default.conf
  /var/lib/lxc           => $HOME/.local/share/lxc
  /var/lib/lxcsnaps      => $HOME/.local/share/lxcsnaps
  /var/cache/lxc         => $HOME/.cache/lxc


guideline
  edit /etc/lxc/lxc-usernet 
    name_of_user veth lxcbr0 2
    # 若使用 lxcbr0 以外的橋接器，則需同步修改 /etc/lxc/lxc-usernet 及 /path/to/lxc/config

  grep 'name_of_user' /etc/sub?id
    /etc/subgid => name_of_user:100000:65536
    /etc/subuid => name_of_user:100000:65536
    # 對 user 開啟 host uid 權限，自 100000 起，共 65536 個 uid，該範圍 uid 供 container user_account 映射使用。不在範圍內的 host id 不得映射。gid 映射方式亦同。

  mkdir -p $HOME/.config/lxc

  cp /etc/lxc/default.conf $HOME/.config/lxc && edit $HOME/.config/lxc/default.conf
    lxc.idmap = u 0 100000 65536
    lxc.idmap = g 0 100000 65536

  ln -s /home/lxcu $HOME/.local/share/lxc

  chmod a+x $HOME/.local/share/lxc, each folder in the chain should be +x




Memory and CPU Limit
====================
設定記憶體上限
  sudo lxc-cgroup -n 虛擬機器名稱 memroy.limit_in_bytes 256M

設定 CPU 使用上限
  sudo lxc-cgroup -n 虛擬機器名稱 cpu.shares 512	數字最高為 1024，不帶數字參數就會顯示設定值
  sudo lxc-cgroup -n ubuntu-lxc-vm cpuset.cpus 0,3	使用第 0、4顆 CPU

以設定檔 /var/lib/lxc/虛擬機器名稱/config 設定記憶體及 CPU 上限
  lxc-cgroup.memroy.limit_in_bytes = 256M
  lxc-cgroup.cpu.shares = 512
  lxc-cgroup.cpu.shares = 0,3




搬移 container
====================
先在目的電腦上建新的同名 container
  sudo lxc-create -t ubuntu -n name_of_machine

停止來源電腦 Container
  sudo lxc-stop -n name_of_machine

以 rsync 指令，透過 ssh 傳送 Container
  sudo rsync -av --numberic-ids /path/to/name_of_machine -e ssh name_of_user@url_of_destination:/path/to/clone
  # 要保留檔案的使用者 ID，一定要使用 rsync 的 --numberic-ids 選項來複製檔案
  # name_of_user@url_of_destination: 如 xxx@192.168.nn.nn
  # /path/to/clone: 如 /home/virt/lxdu/u1606u

在目的電腦以複製過來的 container 資料夾覆蓋掉先前 lxc-create 建立的 container 資料夾
  sudo rmdir /path/to/name_of_machine
  sudo cp -rf /path/to/clone/. /path/to/name_of_machine




Creat LXC Container
====================
sudo lxc-create -t ubuntu -n name_of_machine
  用預設範本建立 Container	，預設的帳號和密碼都是「ubuntu」

sudo lxc-create -t download -n name_of_machine -- -d ubuntu -r jammy -a amd64
  用下載範本建立 Container，建立和操作 Container 時，都不需要用到 sudo 指令
  # sudo lxc-create -n centos_lxc -t centos -- -R 7 -a x86_64




Install Packages
====================
sudo apt install lxc bridge-utils uidmap systemd
  # systemd uidmap 是 Unprivileged Mode 需要
mkdir $HOME/.config/lxc $HOME/.local/share/lxc $HOME/.cache/lxc
ls /usr/share/lxc/templates




Memo Data
====================
  lxc容器中無法訪問宿主機物理設備問題解決 :: https://blog.csdn.net/a2591748032/article/details/130961891
  [PVE] 非特权LXC容器挂载宿主机核显实现Plex硬解视频 - 知乎 :: https://zhuanlan.zhihu.com/p/29306228339
    暴力直通法：創建一個 VM，將宿主機核顯直通給 VM
    SR-IOV + 直通法：通過 SR-IOV 技術，將宿主機核顯分隔為多個子設備，直通給不同的 VM
    特權型 LXC + 核顯設備共用類
    非特權型 LXC + 核顯設備共用 <= 建議使用


****

To play music in an LXC container, provide the container with access to a sound device by mounting the host's audio device node (like /dev/snd) into the container, and allow the container's user to access it. For a more robust setup, especially for desktop applications, integrate the container with the host's PulseAudio server by exporting the PULSE_SERVER environment variable within the container, pointing to a Unix socket

Method 1: Direct Device Access (for ALSA applications)
This method is suitable for applications that use ALSA (Advanced Linux Sound Architecture) directly
1.Configure the Container for Sound Device Access:
  lxc.cgroup.devices.allow = c 116:* rwm
  lxc.mount.entry = /dev/snd dev/snd none bind,optional,create=dir

2.Map the Audio Device
  Ensure the user inside the container is in the audio or audio group. You may need to map the host and container user/group IDs if using unprivileged container.

3.Test with an ALSA Player
  Install an ALSA player inside the container, such as alsa-utils.
  Use a command like aplay -L to list available sound devices and play a sound file

Method 2: Using PulseAudio Socket (for Desktop Applications)
1.Configure PulseAudio on the Host:
  Load the PulseAudio module for TCP connections (if needed):
    pactl load-module module-native-protocol-tcp
2.Configure the Container to Use the PulseAudio Socket:
  # Example for a user with UID 1000 on both host and container
  PulseSocket1: bind
  container
  connect
  unix:/run/user/1000/pulse/native
  listen
  unix:/home/your_user_inside_container/pulse-socket
  security.gid: "1000"
  security.uid: "1000"
  uid: "1000"
  gid: "1000"
  mode: "0777"
  type: proxy.
3.Alternatively, set the PULSE_SERVER environment variable for the user inside the container
  export PULSE_SERVER=unix:/home/your_user_inside_container/pulse-socket


1. Grant Permissions
   Ensure the container's user has access to the PulseAudio socket by making the user a member of the host's and container's audio or pulse group.
2. Launch the Application
   Run the application inside the container, and it should now route its audio through the PulseAudio server.


****


lxc.cgroup2.devices.deny:將設備從白名單中刪除；
lxc.cgroup2.devices.allow = <type> <major>:<minor> <access> :將設備添加到白名單中；
  其中 type 可選值為：a , c , b ，a表示所有設備，c為字元設備，b為塊設備。
  major 和 minor 為設備的主從設備號，用整數或者*表示所有設備。
  access 為訪問權限的結合r(讀)，w(寫)和m(mknod)。


****

lxc.mount.entry: <source> <destination> <filesystem_type> <options> <dump> <pass>
lxc.mount.entry: tmpfs dev/shm tmpfs size=8G,nosuid,nodev,noexec,create=dir 0 0
  tmpfs (source): This indicates that the source of the mount is a tmpfs filesystem.
  dev/shm (destination): This is the path inside the container where the tmpfs will be mounted.
  tmpfs (filesystem_type): Specifies the filesystem type as tmpfs.
  size=8G,nosuid,nodev,noexec,create=dir (options):
    size=8G: Limits the size of the tmpfs to 8 Gigabytes.
    nosuid: Prevents set-user-ID and set-group-ID bits from taking effect.
    nodev: Prevents interpretation of character or block special devices.
    noexec: Prevents execution of binaries on the mounted filesystem.
    create=dir: Ensures the destination directory is created if it does not exist.
  0 (dump): Specifies whether the filesystem should be dumped by dump utility (usually 0 for tmpfs).
  0 (pass): Specifies the order in which filesystems are checked at boot time by fsck (usually 0 for tmpfs).


****

lxc.idmap = <type> <container_id_start> <host_id_start> <count>
  <type>: Specifies whether it's a UID mapping (u) or a GID mapping (g).
  <container_id_start>: The starting UID/GID within the container.
  <host_id_start>: The starting UID/GID on the host system that the container's IDs will map to.
  <count>: The number of consecutive UIDs/GIDs to map.
lxc.idmap = u 0 100000 65536


****

lxc.net.<index>.type = proxy
lxc.net.<index>.listen.protocol = <protocol>
lxc.net.<index>.listen.address = <host_ip_or_0.0.0.0>
lxc.net.<index>.listen.port = <host_port>
lxc.net.<index>.connect.protocol = <protocol>
lxc.net.<index>.connect.address = <container_ip_or_127.0.0.1>
lxc.net.<index>.connect.port = <container_port>
  <index>: A numerical index for the network device (e.g., 0, 1, etc.).
  lxc.net.<index>.type = proxy: Specifies the device type as a proxy.
  lxc.net.<index>.listen.protocol = <protocol>: The protocol the proxy listens on the host (e.g., tcp, udp).
  lxc.net.<index>.listen.address = <host_ip_or_0.0.0.0>: The IP address on the host where the proxy listens. Use 0.0.0.0 to listen on all host interfaces.
  lxc.net.<index>.listen.port = <host_port>: The port on the host where the proxy listens.
  lxc.net.<index>.connect.protocol = <protocol>: The protocol used to connect to the container.
  lxc.net.<index>.connect.address = <container_ip_or_127.0.0.1>: The IP address within the container to which the proxy connects. Use 127.0.0.1 if the service is listening on the loopback interface inside the container.
  lxc.net.<index>.connect.port = <container_port>: The port within the container to which the proxy connects.

# lxc config device add <container_name> <device_name> proxy listen=<protocol>:<host_ip>:<host_port> connect=<protocol>:<container_ip>:<container_port>
# lxc config device add myWeb-container myWebPort80 proxy listen=tcp:0.0.0.0:80 connect=tcp:127.0.0.1:80


****

lxc.mount.entry: /run/user/[UID]/pipewire-0 tmp/pipewire-0 none bind,optional,create=file
yum install pipewire pipewire-alsa alsa-utils
speaker-test 
# you should hear a sound here ;; pw-cli ls

----
how to connect Pipewire sound to a container? :: https://forums.fedoraforum.org/showthread.php?329753-how-to-connect-Pipewire-sound-to-a-container
cp /usr/share/pipewire/pipewire-pulse.conf /etc/pipewire/
vi /etc/pipewire/pipewire-pulse.conf
...
    {   name = libpipewire-module-protocol-pulse
        args = {
        # the addresses this server listens on
            server.address = [
                # type same as in /etc/pulse/*.
                "unix:/tmp/pulse-socket"

...
mp0: /target/test,mp=/target
mp1: /mnt/pve/freenas,mp=/mnt/pve/freenas
mp0,mp1  ---> PVE 主機上的目錄 ,
mp       ---> LXC 虛擬機上要掛載的目錄 (不用手動建立，掛載後會自動建立. 若事先建立，系統會將原來的目錄遮蔽.)

格式：mp<掛載點序號>: <宿主機目錄>,mp=容器目錄,參數

----
lxc config device add <container_name> pipewire-socket unix-socket path=/run/user/1000/pipewire-0 state=present connect=true
# lxc config device add <instance> <device-name> <device-type> [key=value...]
lxc-cgroup -n xxx devices.list #顯示允許容器xxx使用的設備列表
lxc-cgroup -n xxx cpuset.cpus “0,3” #將處理器0和3分配給容器xxx


lxc.mount.entry: tmpfs tmp tmpfs defaults
lxc.mount.entry: /tmp/.X11-unix tmp/.X11-unix none bind,optional,create=dir
lxc.mount.entry: /run/user/[UID]/pipewire-0 tmp/pipewire-0 none bind,optional,create=file => mp0: /run/user/[UID],mp=/srv/sockets
lxc.mount.entry: /dev/snd dev/snd <device_node> <target_node> bind,optional,create=dir


lxc.cgroup2.devices.allow + lxc.mount.entry + idmap
lxc.cgroup2.devices.allow: c 10:200 rwm
lxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file


lxc.mount.entry: /dev/snd dev/snd none bind,optional,create=dir
lxc.cgroup2.devices.allow: c 226:* rwm # Example for a common sound card device
lxc.idmap: g 2999 2999 1


----
LXC Bind Mounts :: http://www.gienginali.idv.tw/modules/tad_book3/page.php?tbdsn=502
Mounting sockets in lxc :: https://discuss.linuxcontainers.org/t/mounting-sockets-in-lxc/17711

----


To enable the PipeWire module for TCP connections, specifically for PulseAudio compatibility over a network, you need to configure the pipewire-pulse service to load the module-native-protocol-tcp module.

Steps to enable PipeWire TCP connections:
Edit the PipeWire PulseAudio configuration.
Locate the pipewire-pulse.conf file. This file is typically found in /etc/pipewire/ or ~/.config/pipewire/. You might need to create a local copy in ~/.config/pipewire/pipewire-pulse.conf to avoid overwriting system-wide configurations during updates. /usr/share/pipewire

Uncomment or add the TCP server address:
  Within the context.properties section of pipewire-pulse.conf, find the server.address entry. Uncomment the line that specifies the TCP address, typically tcp:4713.
    server.address = [ "unix:native" # "tcp:4713" ] => server.address = [ "unix:native" "tcp:4713" ]
  You can also use a more verbose syntax to configure additional options like max-clients or client.access if needed:
    server.address = [
        { address = "tcp:4713"
          # address.max-clients = 64
          # listen-backlog = 32
          # client.access = "restricted"
        }
    ]

Ensure module-native-protocol-tcp is loaded:
  This module is usually loaded automatically when you specify a TCP address in server.address. However, if you encounter issues, you can explicitly add it to the context.modules section:
    context.modules = [
        # ... other modules ...
        { name = libpipewire-module-protocol-native }
        { name = libpipewire-module-protocol-pulse.args = { } }
        { name = libpipewire-module-native-protocol-tcp.args = { } } # Add this line if not present
    ]

Restart PipeWire PulseAudio service.
  After modifying the configuration, restart the pipewire-pulse service to apply the changes:
    systemctl --user restart pipewire-pulse.service

  If you are running a full PipeWire stack, you might also need to restart wireplumber and pipewire services:
    systemctl --user restart wireplumber pipewire pipewire-pulse

Important considerations:
  Firewall:
    Ensure that the TCP port (default 4713) is open in your firewall on the server machine to allow incoming connections.
  Security:
    Be aware that enabling TCP connections can expose your audio server to the network. Consider using client.access = "restricted" and configuring appropriate permissions if security is a concern.
  Client Configuration:
    On the client side, you would typically use pactl load-module module-tunnel-sink or similar commands to connect to the remote PipeWire PulseAudio server.
